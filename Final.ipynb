{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6263606-454d-4c37-996d-74d0305b7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pandas\n",
    "#!pip install -U scikit-learn\n",
    "#!pip install -U matplotlib\n",
    "#!pip install tensorflow\n",
    "#!pip install -U transformers\n",
    "#!pip install -U tf-keras\n",
    "#!pip install -U focal-loss\n",
    "#!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bcc03-604a-4f90-b925-6abcf379fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer, TFDistilBertModel\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dropout, Dense\n",
    "import re,string,unicodedata\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7981d78-5703-4156-804c-4ce5e8c783d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/IMDB Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ef033-4c68-4843-a951-ad3085691456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Positive and Negative Sentiment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9e82d-ef25-47de-bc42-6bf7f6d4fd26",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2a124-9e93-4023-9f03-cf9f9221f271",
   "metadata": {},
   "source": [
    "### Deleting duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42c80c-d0fb-4183-b158-24796cd28bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb83e-04ff-4d82-b2b6-a878b8ca2b9a",
   "metadata": {},
   "source": [
    "### Removing abbreviations and html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cfeda-502d-45f4-a6c9-5b093d1f19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n",
    "           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n",
    "           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "           \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "#function to clean data\n",
    "def clean_text(text,lemmatize = True):\n",
    "    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n",
    "    text = soup.get_text()\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n",
    "    emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_clean.sub(r'',text)\n",
    "    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n",
    "    text = re.sub(r'http\\S+', '', text) #remove urls\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    #tokens = re.split('\\W+', text) #create tokens\n",
    "    if lemmatize:\n",
    "        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n",
    "    else:\n",
    "        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce365fa-e057-40fb-83b8-35fd8367d2d7",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca91517-f596-4b5b-ae36-28619e70bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment to numerical value\n",
    "df['sentiment'] = (df['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "# Preprocess text\n",
    "df['review'] = df['review'].apply(clean_text, lemmatize=False)\n",
    "\n",
    "# Split into train and test\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ce3e0-1185-4069-9f17-3926a267d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfe149-235c-4dbf-b84f-c528b6a0e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total samples:\", len(df))\n",
    "print(\"Training samples:\", len(train))\n",
    "print(\"Validation samples:\", len(val))\n",
    "print(\"Test samples:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c1902-14cd-4657-97bd-054d2d1702ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was running out of gpu memory (only have 3Gb) so this is hard-coded \n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d1d9b-ef86-4f2e-a1e9-c60bd4118bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_length=128):\n",
    "    text_list = texts.tolist()\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        text_list, \n",
    "        max_length=max_length, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "# Tokenize data\n",
    "Xtrain_ids, Xtrain_mask = tokenize_texts(train['review'], max_seq_len)\n",
    "ytrain_array = train['sentiment'].values\n",
    "\n",
    "Xval_ids, Xval_mask = tokenize_texts(val['review'], max_seq_len)\n",
    "yval_array = val['sentiment'].values\n",
    "\n",
    "Xtest_ids, Xtest_mask = tokenize_texts(test['review'], max_seq_len)\n",
    "ytest_array = test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40bdc3-4c7f-4a5f-bff2-19bf1ed157b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(input_ids, attention_mask, labels, batch_size=32):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': tf.convert_to_tensor(input_ids, dtype=tf.int32), \n",
    "            'attention_mask': tf.convert_to_tensor(attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        labels\n",
    "    )).batch(batch_size)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_tf_dataset(Xtrain_ids, Xtrain_mask, ytrain_array)\n",
    "val_dataset = create_tf_dataset(Xval_ids, Xval_mask, yval_array)\n",
    "test_dataset = create_tf_dataset(Xtest_ids, Xtest_mask, ytest_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bfd5e-dada-41e9-91f5-14b1eb65ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_model(transformer, max_len):\n",
    "    input_ids_layer = Input(shape=(max_len,), dtype='int32', name='input_ids')\n",
    "    attention_mask_layer = Input(shape=(max_len,), dtype='int32', name='attention_mask')\n",
    "    \n",
    "    transformer.trainable = False\n",
    "\n",
    "    def transformer_model(inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        return transformer(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    # I have no idea what's going on here. When I switched from my\n",
    "    # laptop to my computer, I started getting a type error.\n",
    "    # I couldnt find anything online about the type error I was getting,\n",
    "    # so I asked claude and got this ...\n",
    "    bert_outputs = tf.keras.layers.Lambda(\n",
    "        transformer_model, \n",
    "        output_shape=(max_len, transformer.config.hidden_size)\n",
    "    )([input_ids_layer, attention_mask_layer])\n",
    "    \n",
    "    \n",
    "    x = GlobalAveragePooling1D()(bert_outputs)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs={'input_ids': input_ids_layer, 'attention_mask': attention_mask_layer}, \n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ba8a4-a6e9-4e02-913a-ab35aa75df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DistilBERT model\n",
    "transformer = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4c4be-80d5-4547-93f5-f5f2f3dd4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_bert_model(transformer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07958c60-8a30-4905-8d40-1a14884b96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(test_dataset)\n",
    "ypred = ypred>0.5\n",
    "\n",
    "s2=accuracy_score(ytest_array,ypred)\n",
    "print(\"Random Forest Classifier Accuracy :\", \"{:.2f}%\".format(100*s2))\n",
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(ytest_array, ypred)\n",
    "sns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85c856-a5d2-4ee8-aeec-a13f15ea0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bert_report = classification_report(ytest_array, ypred)\n",
    "print(base_bert_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7ce83-da2c-4751-9c96-d260f5cdea83",
   "metadata": {},
   "source": [
    "# Fine-tuning DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303fd36e-517b-4292-90a6-ee4849818f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684e21e-7059-42c4-b1a4-11e921259255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.save('./checkpoints/fine_tuned.h5')\n",
    "\n",
    "#model = load_model('./checkpoints/fine_tuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ed743-5f1f-4446-b8bb-042976722cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearningCurve(history,epochs):\n",
    "  epochRange = range(1,epochs+1)\n",
    "  fig , ax = plt.subplots(1,2,figsize = (10,5))\n",
    "  \n",
    "  ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n",
    "  ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n",
    "  ax[0].set_title('Training and Validation accuracy')\n",
    "  ax[0].set_xlabel('Epoch')\n",
    "  ax[0].set_ylabel('Accuracy')\n",
    "  ax[0].legend()\n",
    "  ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n",
    "  ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n",
    "  ax[1].set_title('Training and Validation loss')\n",
    "  ax[1].set_xlabel('Epoch')\n",
    "  ax[1].set_ylabel('Loss')\n",
    "  ax[1].legend()\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a722e-e261-4c8d-98bf-d3cf0b8e58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(history, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba8910-51e8-4500-95a2-c014e9c9f173",
   "metadata": {},
   "source": [
    "The graphs show that there is very minimal overfitting since the curves for loss and accuracy are both better in validation than training.\n",
    "\n",
    "This may indicate that the model is underfitting, but the results are so close that they are negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5089d-19fd-417c-a674-d3df6564fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(test_dataset)\n",
    "ypred = ypred>0.5\n",
    "\n",
    "s2=accuracy_score(ytest_array,ypred)\n",
    "print(\"Random Forest Classifier Accuracy :\", \"{:.2f}%\".format(100*s2))\n",
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(ytest_array, ypred)\n",
    "sns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce834b4b-e720-48ed-8227-178f5a2e280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_bert_report = classification_report(ytest_array, ypred)\n",
    "print(ft_bert_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495b498-47f9-400d-85a1-89b8474dc304",
   "metadata": {},
   "source": [
    "### Comparing base DistilBERT vs fine-tuned DistilBERT\n",
    "\n",
    "The base model performs very poorly and doesn't show any ability to correctly classify the sentiment of reviews, while the fine-tuned model performs very well, correctly classifying reviews 81% of the time.\n",
    "\n",
    "The fine-tuned model produces more false positives which is likely a side effect of the base model almost always predicting that the reviews were positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a793007-91d8-4563-9297-1f2b979d30e8",
   "metadata": {},
   "source": [
    "# Classical Machine Learning Model: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ce6c7-9459-4a56-8213-e547558a7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-splitting data since we don't need validation set\n",
    "train, test= train_test_split(df, test_size=0.2, random_state=42)\n",
    "Xtrain, ytrain = train['review'], train['sentiment']\n",
    "Xtest, ytest = test['review'], test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c59978-a06b-4fea-af0d-6d621665bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding data for Random Forest\n",
    "tfidf_vect = TfidfVectorizer() #tfidfVectorizer\n",
    "Xtrain_tfidf = tfidf_vect.fit_transform(Xtrain)\n",
    "Xtest_tfidf = tfidf_vect.transform(Xtest)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer() # CountVectorizer\n",
    "Xtrain_count = count_vect.fit_transform(Xtrain)\n",
    "Xtest_count = count_vect.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82d8a5-b2aa-4b11-8ab9-eae34ae6855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(Xtrain_tfidf, ytrain)\n",
    "p2=rf.predict(Xtest_tfidf)\n",
    "s2=accuracy_score(ytest,p2)\n",
    "print(\"Random Forest Classifier Accuracy :\", \"{:.2f}%\".format(100*s2))\n",
    "ConfusionMatrixDisplay.from_estimator(rf,Xtest_tfidf,ytest)\n",
    "#plot_confusion_matrix(rf, Xtest_tfidf, ytest,cmap = 'Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6a55e-2caf-4e54-bcf5-f5c4e638f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_report = classification_report(ytest, p2)\n",
    "print(rf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d223cef-911e-4486-8c2e-41dd2b729b3f",
   "metadata": {},
   "source": [
    "# Classification Reports\n",
    "\n",
    "| Model | Precision (negative) | Recall (negative) | F1-Score (negative) | Precision (positive) | Recall (positive) | F1-Score (positive) |\n",
    "|---|---|---|---|---|---|---|\n",
    "| DistilBERT (base) | 0.28 | 0.04 | 0.07 | 0.48 | 0.89 | 0.63 |\n",
    "| DistilBERT (fine-tuned) | 0.84 | 0.77 | 0.80 | 0.79 | 0.85 | 0.82 |\n",
    "| Random Forest Classifier | 0.85 | 0.86 | 0.85 | 0.86 | 0.85 | 0.85 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cfbc9-268b-4866-a9c1-340bcf96851a",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "The base DistilBERT model showed lower precision than choosing at random so it was a complete failure. The fine-tuned DistilBERT model showed comparable precision to the Random Forest Classifier (RF), but was slightly lower with its predictions for positive reviews.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "The base DistilBERT model scored very low in it's recall rate for negative reviews, which is expected since it classified almost all of the reviews as positive. It's performace with positive reviews looks good on paper, but since the model almost exlusively predicted that reviews were positive, this metric means almost nothing. A model that predicted every single review to be poitive would recieve a recall of 1.00 despite being incredibly flawed\n",
    "\n",
    "The fine-tuned DistilBERT model had a slightly lower recall rate with negative reviews than RF, but a similar rate to RF for positive reviews. This finding shows that the RF model would be better at reliably identifying negative reviews than the fine-tuned model.\n",
    "\n",
    "#### F1-Score\n",
    "\n",
    "The F1-Scores of the models rank in the following order: base DistilBERT, fine-tuned DistilBERT, RF. Overall, RF was better in almost every metric, but the fine-tuned model was a close second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c44d47-2e4a-4c41-8c5a-c8951c20da9a",
   "metadata": {},
   "source": [
    "# Time Complexity\n",
    "---\n",
    "### Random Forest vs. Base DistilBERT\n",
    "\n",
    "The base distilbert model took roughly the same amount of time to get running as the random forest classifier, but produced very bad results. This means there is no advantage to using the llm without fine-tuning.\n",
    "\n",
    "### Random Forest vs. Fine-tuned DistilBERT\n",
    "\n",
    "The random forest classifier (RF) performed better than the fine-tuned distilBERT model despite only taking roughly 2 minutes to train vs the 15+ minutes (4 hrs w/o gpu) of the fine-tuned distilBERT model.\n",
    "\n",
    "One thing to note is that in the preprocessing stage, we remove all non-descriptive words. Removing these words from the input data inherently gives RF an advantage, because llm's are better at looking at the context of a sentence to determine their output.\n",
    "\n",
    "It would be interesting to see the performance difference between a larger llm (or distilBERT with more training) and RF without the data preprocessing step that removes \"unnecessary\" words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
